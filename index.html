<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.svg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://barchid.github.io/" target="_blank">Sami BARCHID</a> <sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://pro.univ-lille.fr/jose-mennesson" target="_blank">Jos√© MENNESSON</a> <sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="https://pro.univ-lille.fr/chabane-djeraba" target="_blank">Chaabane DJ√âRABA</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup> Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL
                      <br>
                      <sup>2</sup> IMT Nord Europe, Institut Mines-T√©l√©com, Centre for Digital Systems
                      <br>
                      <br>
                      <a href="https://tub-rip.github.io/eventvision2023/">4th International Workshop on Event-Based Vision</a> - CVPR 2023
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Barchid_Exploring_Joint_Embedding_Architectures_and_Data_Augmentations_for_Self-Supervised_Representation_CVPRW_2023_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/appendix.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Barchid/exploring_event_ssl" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper proposes a self-supervised representation learning (SSRL) framework for event-based vision, which leverages various lightweight convolutional neural networks (CNNs) including 2D-, 3D-, and Spiking CNNs. The method uses a joint embedding architecture to maximize the agreement between features extracted from different views of the same event sequence. Popular event data augmentation techniques are employed to design an efficient augmentation policy for event-based SSRL, and we provide novel data augmentation methods to enhance the pretraining pipeline. Given the novelty of SSRL for event-based vision, we elaborate standard evaluation protocols and use them to evaluate our approach. Our study demonstrates that pretrained CNNs acquire effective and transferable features, enabling them to achieve competitive performance in object or action recognition across various commonly used event-based datasets, even in a low-data regime. This paper also conducts an experimental analysis of the extracted features regarding the Uniformity-Tolerance tradeoff to assess their quality, and measure the similarity of representations using linear Center Kernel Alignement. These quantitative measurements reinforce our observations from the performance benchmarks and show substantial differences between the learned representations of all types of CNNs despite being optimized with the same approach.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-link">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üëã Event-Based Self-Supervised Representation Learning <span style="font-weight: normal; font-style: italic;">(SSRL)</span></h2>
        <div class="content has-text-justified">
          <p class="subtitle">‚ö†Ô∏è Many event-based datasets have a limited number of labeled samples</p>
          <p class="subtitle">üëé It presents challenges for the development of event vision algorithms</p>
          <p class="subtitle">üëç SSRL is a good solution for reducing the reliance on labeled data</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">‚öôÔ∏è Proposed Approach</h2>
        <div class="content has-text-justified">
          
          <p class="subtitle">
            We propose a straightforward and efficient framework for event-based SSRL using different types of convolutional encoders, including 2D-/3D-/Spiking CNNs in a joint embedding architecture.
          </p>
          

          <figure class="image">
            <img src="static/images/approach.png">
          </figure>

          <p class="subtitle">Our method employs event data augmentations (EDAs) for the pretraining of a specific convolutional encoder.</p>
          <p class="subtitle">Two variants of the joint embedding architecture are designed to enhance the performance of CSNNs</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-danger" >
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üë• Study on Event Data Augmentations <span style="font-weight: normal; font-style: italic;">(EDAs)</span></h2>
        <div class="content has-text-justified">
          <p>
            Our method depends on the distribution of EDAs for obtaining distorted views of input events, making the design of an effective EDA distribution crucial. In this paper, we examine existing and novel EDAs to create a comprehensive distribution that enhances the efficiency of our approach and future research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small is-danger">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/normal.gif" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Original
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/background_activity.gif" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Background Activity Noise
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/polarity_flip.gif" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Flip Polarity
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/crop.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Crop
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/static_translation.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Static Translation
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/static_rotation.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Static Rotation
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/dynamic_translation.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Dynamic Translation <b>(ours)</b>
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/dynamic_rotation.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Dynamic Rotation <b>(ours)</b>
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/cutout.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Cutout
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/drop_by_time.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Drop by Time
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/random_drop.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Random Drop
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/event_copy.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        EventCopy <b>(ours)</b>
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">‚öñÔ∏è Evaluation Protocols</h2>
        <div class="content has-text-justified">
          
          <p>
            Due to the recent emergence of event-based SSRL, there is a lack of established evaluation protocols for assessing the performance of event-based vision models pretrained with a SSRL method. To address this, we introduce three standardized protocols focusing on object and activity recognition.
          </p>
          
          <p class="subtitle">Linear Evaluation Protocol</p>
          <figure class="image">
            <img style="width: 256px;" src="static/images/linear.png">
          </figure>
          <p>üéØ Efficiency of learned features</p>

          <p class="subtitle">Semi-Supervised Training</p>
          <figure class="image">
            <img style="width: 256px;" src="static/images/semisup.png">
          </figure>
          <p>üéØ Ability to reduce the reliance on labels</p>

          <p class="subtitle">Transer to Another Dataset</p>
          <figure class="image">
            <img style="width: 256px;" src="static/images/transfer.png">
          </figure>
          <p>üéØ Transferability of learned features</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-primary">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üîé Analysis of Learned Features</h2>
        <div class="content has-text-justified">
          
          <p>
            Beyond performance benchmarks, we quantitatively evaluate the learned representations of all convolutional encoders according to the <b>Uniformity-Tolerance</b> trade-off and the similarity assesment with <b>linear CKA</b>.
          </p>
          
          <figure class="image">
            <img src="static/images/uf.png">
          </figure>

          <figure class="image">
            <img src="static/images/cka.png">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üí™ Exciting Perspectives</h2>
        <div class="content has-text-justified">
          
          <p class="subtitle">
            Our event-based SSRL method achieves competitive performance compared to fully supervised models from the state-of-the-art, despite using lighter encoders and without utilizing the full datasets for training.
          </p>
          
          <figure class="image">
            <img src="static/images/tab1.png">
          </figure>

          <figure class="image">
            <img src="static/images/tab2.png">
          </figure>

          <figure class="image">
            <img src="static/images/tab3.png">
          </figure>

          <figure class="image">
            <img src="static/images/tab4.png">
          </figure>

          <figure class="image">
            <img src="static/images/tab5.png">
          </figure>

          <h1>Check the paper for more!</h1>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Performance Benchmarks</h2>
      
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/cvpr_poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @inproceedings{barchid2023exploring,
        title={Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision},
        author={Barchid, Sami and Mennesson, Jos{\'e} and Dj{\'e}raba, Chaabane},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={3902--3911},
        year={2023}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
